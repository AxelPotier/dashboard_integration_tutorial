{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1fd3e0",
   "metadata": {},
   "source": [
    "# Dashboard tutorial using streamlit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b610f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Series constructor called with unsupported type 'ndarray' for the `values` parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m     df.write_csv(filename, separator=\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m,include_header=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Ensure comma is used as the delimiter\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Generate 100,000 rows of data with random order_date and save to CSV\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m generate(\u001b[32m3\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msales_data.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(nrows, filename)\u001b[39m\n\u001b[32m     55\u001b[39m columns = {\n\u001b[32m     56\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33morder_id\u001b[39m\u001b[33m\"\u001b[39m: np.arange(nrows),\n\u001b[32m     57\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33morder_date\u001b[39m\u001b[33m\"\u001b[39m: order_dates,\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m\"\u001b[39m: price * quantity,\n\u001b[32m     66\u001b[39m }\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Create Polars DataFrame and write to CSV with explicit delimiter\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m df = pl.DataFrame(columns)\n\u001b[32m     70\u001b[39m df.write_csv(filename, separator=\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m,include_header=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\axelp\\anaconda3\\envs\\dashb_tutorial_env\\Lib\\site-packages\\polars\\dataframe\\frame.py:358\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, schema, schema_overrides, strict, orient, infer_schema_length, nan_to_null)\u001b[39m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28mself\u001b[39m._df = dict_to_pydf(\n\u001b[32m    354\u001b[39m         {}, schema=schema, schema_overrides=schema_overrides\n\u001b[32m    355\u001b[39m     )\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m     \u001b[38;5;28mself\u001b[39m._df = dict_to_pydf(\n\u001b[32m    359\u001b[39m         data,\n\u001b[32m    360\u001b[39m         schema=schema,\n\u001b[32m    361\u001b[39m         schema_overrides=schema_overrides,\n\u001b[32m    362\u001b[39m         strict=strict,\n\u001b[32m    363\u001b[39m         nan_to_null=nan_to_null,\n\u001b[32m    364\u001b[39m     )\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, Sequence)):\n\u001b[32m    367\u001b[39m     \u001b[38;5;28mself\u001b[39m._df = sequence_to_pydf(\n\u001b[32m    368\u001b[39m         data,\n\u001b[32m    369\u001b[39m         schema=schema,\n\u001b[32m   (...)\u001b[39m\u001b[32m    374\u001b[39m         nan_to_null=nan_to_null,\n\u001b[32m    375\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\axelp\\anaconda3\\envs\\dashb_tutorial_env\\Lib\\site-packages\\polars\\_utils\\construction\\dataframe.py:159\u001b[39m, in \u001b[36mdict_to_pydf\u001b[39m\u001b[34m(data, schema, schema_overrides, strict, nan_to_null, allow_multithreaded)\u001b[39m\n\u001b[32m    146\u001b[39m     data_series = [\n\u001b[32m    147\u001b[39m         pl.Series(\n\u001b[32m    148\u001b[39m             name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m column_names\n\u001b[32m    155\u001b[39m     ]\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    157\u001b[39m     data_series = [\n\u001b[32m    158\u001b[39m         s._s\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m _expand_dict_values(\n\u001b[32m    160\u001b[39m             data,\n\u001b[32m    161\u001b[39m             schema_overrides=schema_overrides,\n\u001b[32m    162\u001b[39m             strict=strict,\n\u001b[32m    163\u001b[39m             nan_to_null=nan_to_null,\n\u001b[32m    164\u001b[39m         ).values()\n\u001b[32m    165\u001b[39m     ]\n\u001b[32m    167\u001b[39m data_series = _handle_columns_arg(data_series, columns=column_names, from_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    168\u001b[39m pydf = PyDataFrame(data_series)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\axelp\\anaconda3\\envs\\dashb_tutorial_env\\Lib\\site-packages\\polars\\_utils\\construction\\dataframe.py:388\u001b[39m, in \u001b[36m_expand_dict_values\u001b[39m\u001b[34m(data, schema_overrides, strict, order, nan_to_null)\u001b[39m\n\u001b[32m    385\u001b[39m     updated_data[name] = s\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arrlen(val) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m _is_generator(val):\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     updated_data[name] = pl.Series(\n\u001b[32m    389\u001b[39m         name=name,\n\u001b[32m    390\u001b[39m         values=val,\n\u001b[32m    391\u001b[39m         dtype=dtype,\n\u001b[32m    392\u001b[39m         strict=strict,\n\u001b[32m    393\u001b[39m         nan_to_null=nan_to_null,\n\u001b[32m    394\u001b[39m     )\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(  \u001b[38;5;66;03m# type: ignore[redundant-expr]\u001b[39;00m\n\u001b[32m    396\u001b[39m     val, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, date, datetime, time, timedelta)\n\u001b[32m    397\u001b[39m ):\n\u001b[32m    398\u001b[39m     updated_data[name] = F.repeat(\n\u001b[32m    399\u001b[39m         val, array_len, dtype=dtype, eager=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    400\u001b[39m     ).alias(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\axelp\\anaconda3\\envs\\dashb_tutorial_env\\Lib\\site-packages\\polars\\series\\series.py:359\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, name, values, dtype, strict, nan_to_null)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    355\u001b[39m     msg = (\n\u001b[32m    356\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSeries constructor called with unsupported type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(values).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    357\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m for the `values` parameter\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    358\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[31mTypeError\u001b[39m: Series constructor called with unsupported type 'ndarray' for the `values` parameter"
     ]
    }
   ],
   "source": [
    "# generate the 1m record CSV file\n",
    "#\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate(nrows: int, filename: str):\n",
    "    names = np.asarray(\n",
    "        [\n",
    "            \"Laptop\",\n",
    "            \"Smartphone\",\n",
    "            \"Desk\",\n",
    "            \"Chair\",\n",
    "            \"Monitor\",\n",
    "            \"Printer\",\n",
    "            \"Paper\",\n",
    "            \"Pen\",\n",
    "            \"Notebook\",\n",
    "            \"Coffee Maker\",\n",
    "            \"Cabinet\",\n",
    "            \"Plastic Cups\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categories = np.asarray(\n",
    "        [\n",
    "            \"Electronics\",\n",
    "            \"Electronics\",\n",
    "            \"Office\",\n",
    "            \"Office\",\n",
    "            \"Electronics\",\n",
    "            \"Electronics\",\n",
    "            \"Stationery\",\n",
    "            \"Stationery\",\n",
    "            \"Stationery\",\n",
    "            \"Electronics\",\n",
    "            \"Office\",\n",
    "            \"Sundry\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    product_id = np.random.randint(len(names), size=nrows)\n",
    "    quantity = np.random.randint(1, 11, size=nrows)\n",
    "    price = np.random.randint(199, 10000, size=nrows) / 100\n",
    "\n",
    "    # Generate random dates between 2010-01-01 and 2023-12-31\n",
    "    start_date = datetime(2010, 1, 1)\n",
    "    end_date = datetime(2023, 12, 31)\n",
    "    date_range = (end_date - start_date).days\n",
    "\n",
    "    # Create random dates as np.array and convert to string format\n",
    "    order_dates = np.array([(start_date + timedelta(days=np.random.randint(0, date_range))).strftime('%Y-%m-%d') for _ in range(nrows)])\n",
    "\n",
    "    # Define columns\n",
    "    columns = {\n",
    "        \"order_id\": list(np.arange(nrows)),\n",
    "        \"order_date\": list(order_dates),\n",
    "        \"customer_id\": list(np.random.randint(100, 1000, size=nrows)),\n",
    "        \"customer_name\": list([f\"Customer_{i}\" for i in np.random.randint(2**15, size=nrows)]),\n",
    "        \"product_id\": list(product_id + 200),\n",
    "        \"product_names\": list(names[product_id]),\n",
    "        \"categories\": list(categories[product_id]),\n",
    "        \"quantity\": list(quantity),\n",
    "        \"price\": list(price),\n",
    "        \"total\": list(price * quantity),\n",
    "    }\n",
    "\n",
    "    # Create Polars DataFrame and write to CSV with explicit delimiter\n",
    "    df = pl.DataFrame(columns)\n",
    "    df.write_csv(filename, separator=',',include_header=True)  # Ensure comma is used as the delimiter\n",
    "\n",
    "# Generate 100,000 rows of data with random order_date and save to CSV\n",
    "generate(3, \"sales_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddcb5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dashb_tutorial_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
